# -*- coding: utf-8 -*-
"""PROJECT-MUSHROOM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HlL6HcRVc-KhlWJVQqqQ1MIoxDa-Flxr

# ***Is your mushroom poisonous?***
"""

import pandas as pd
from skimage.io import imread
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report,ConfusionMatrixDisplay

import warnings
warnings.filterwarnings('ignore')

img =imread('/content/drive/MyDrive/PROJECTS/Project-Mushroom/mushroom pic.png')
plt.figure(figsize=(10, 8))
plt.imshow(img)
plt.axis('off')
plt.show()

"""**There is no simple rule for determining edibility of a mushroom. Yet with the vast possibility of machine learning, one can predict the same and that too with 99% accuracy. My project builds a successful model for predicting whether a mushroom is poisonous or not based on different features. The data corresponds  to 23 species of gilled mushrooms in the Agaricus and Lepiota Family.  Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended.  This latter class was combined with the poisonous one**

data source: https://archive.ics.uci.edu/dataset/73/mushroom

**1. Loading data**
"""

df=pd.read_csv('/content/drive/MyDrive/PROJECTS/Project-Mushroom/secondary_data.csv',delimiter=';')
df

df.head()

df.info()

df.describe()

sns.pairplot(df, hue='class')
plt.show()

"""**2. Checking missing values**"""

df.isna().sum()

"""**3.Handling missing values**

Features with more than half missing values are dropped and rest of the missing values are replaced with mode of those columns.
"""

df.drop(['stem-root','stem-surface','veil-type','veil-color','spore-print-color'],axis=1,inplace=True)

labels=['cap-surface','gill-attachment','gill-spacing','ring-type']
for label in labels:
  df[label]=df[label].fillna(df[label].mode()[0])

df.isna().sum()

"""**4.Converting to numeric data types**"""

df.dtypes

encoder=LabelEncoder()
features=['class', 'cap-shape', 'cap-surface', 'cap-color',
       'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',
        'stem-color', 'has-ring', 'ring-type',
       'habitat', 'season']
for feature in features:
  df[feature]=encoder.fit_transform(df[feature])

df.dtypes

"""**5. Evaluating correlation between features and target**

Features with correlation less than 0.06 are dropped.
"""

df.corr()

plt.figure(figsize=(14, 12))
sns.heatmap(df.corr(),annot=True)
plt.show()

corr=df.corr()
target='class'
target_corr=corr[target]
low_correlation_features = target_corr[abs(target_corr) < 0.06]
print(low_correlation_features)

df.drop(['cap-surface','cap-color','does-bruise-or-bleed','gill-spacing','has-ring','habitat','season'],axis=1,inplace=True)
df

labels=['cap-diameter',	'cap-shape',	'gill-attachment'	,'gill-color'	,'stem-height',	'stem-width',	'stem-color'	,'ring-type']
for label in labels:
  print(label)
  print(df[label].unique())

"""**6.Handling imbalance in dataset**"""

X=df.iloc[:,1:]
y=df.iloc[:,0]

df['class'].value_counts()

us=RandomUnderSampler(random_state=1)
X,y=us.fit_resample(X,y)
y.value_counts()

"""**7.Scaling features to a common range**"""

for column in df.columns:
    if column != 'class':
        plt.figure(figsize=(8, 4))
        sns.histplot(df, x=column, hue='class', multiple="stack", kde=True)
        plt.title(f'Histogram of {column} by Class')
        plt.show()

"""The dataset is mostly normally distributed. So standard scaler is the best option."""

scaler=StandardScaler()
X=scaler.fit_transform(X)

"""**8. Splitting data for training and testing.**"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3,random_state=1)
X_train.shape,y_train.shape,X_test.shape,y_test.shape

"""**9. Building models**"""

for column in df.columns:
    if column != 'class':
        plt.figure(figsize=(8, 4))
        sns.boxplot(x='class', y=column, data=df)
        plt.title(f'Boxplot of {column} by Class')
        plt.show()

"""There are severe cases of outliers. The models SVC,KNN and Naive Bayes are highly sensitive to outliers hence exempted. Using tree based algorithms are the best option."""

dec=DecisionTreeClassifier()
rf=RandomForestClassifier()
ab=AdaBoostClassifier()
gb=GradientBoostingClassifier()
xg=XGBClassifier()

models=[dec,rf,ab,gb,xg]
for model in models:
  print(model)
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  print(classification_report(y_test,y_pred))
  print(ConfusionMatrixDisplay.from_predictions(y_test,y_pred))

"""**10. Choosing the best model.**"""

df1=pd.read_csv('/content/drive/MyDrive/PROJECTS/Project-Mushroom/RandomForest_XGBoost_Comparison.csv')
df1

"""Random forest is the best model here."""

import pickle
pickle.dump(rf,open('model_.sav','wb'))
pickle.dump(scaler,open('scaler_.sav','wb'))
pickle.dump(encoder,open('encoder_.sav','wb'))